# Building makemore Part2: MLP

## link

https://www.youtube.com/watch?v=TCH_1BHY58I


## intro

bigram은 단순한 신경명(단일 선형층) 을 사용해서 만들었다. 이 모델의 한계는 한 문자의 이전 문맥(컨텍스트)만 고려하여 예측의 품질이 좋지 않다.
이때, 더 긴 텍스트를 사용할 때 문제점이 나타난다

예: 2문자 컨텍스트 = 729(27*27)가지, 3문자 컨텍스트 = 20,000(27*27*27)가지

따라서, MLP 모델을 도입하여 다음 문자 예측에 사용할 것이다. 

## Bengio et al. 2003 (MLP language model) paper walkthrough

### 1. 논문 소개
- 이 논문은 다층 퍼셉트론(MLP)을 사용한 언어 모델링에 관한 중요한 연구임
- 최초의 논문은 아니지만, 이 분야에서 매우 영향력 있는 논문으로 자주 인용됨
- 19페이지 분량으로, 상세하고 읽기 쉬운 내용을 담고 있음

### 2. 이전 강의와의 연결
- 이전 강의에서 다룬 bigram 모델의 한계를 극복하기 위한 접근법
- bigram 모델은 한 문자의 이전 컨텍스트만 고려하여 예측 품질이 낮았음

### 3. 모델 구조
- **어휘 크기**: 17,000개 단어 (논문 기준)
- **임베딩**: 
  - 각 단어를 30차원 벡터로 표현
  - 17,000개의 점이 30차원 공간에 분포
- **입력**: 이전 3개 단어
- **은닉층**: 크기는 하이퍼파라미터 (예: 100 뉴런)
- **출력층**: 17,000개 뉴런 (다음 단어 예측)

### 4. 임베딩 공간의 특성
- 초기에는 랜덤하게 분포
- 학습을 통해 의미적으로 유사한 단어들이 가까워짐
- 예: 'a'와 'the'가 서로 가까운 위치에 있을 수 있음

### 5. 학습 방법
- 다층 신경망 사용
- 로그 우도 최대화 (이전 강의의 bigram 모델과 동일)
- 역전파를 통한 파라미터 최적화

### 6. 모델의 장점: 일반화 능력
- 예시: "a dog was running in a ___" 문장 예측
- 정확한 문구를 학습하지 않았더라도 유사한 문맥을 통해 예측 가능
- 임베딩 공간을 통한 지식 전이 (예: 'dog'와 'cat'의 유사성)

### 7. 신경망 구조 상세 설명
- **입력층**: 
  - 3개 단어, 각 30차원 → 총 90개 뉴런
  - 각 단어는 0에서 16,999 사이의 정수로 표현
- **임베딩 룩업 테이블 (C)**:
  - 17,000 x 30 크기의 행렬
  - 각 단어 인덱스에 해당하는 행을 추출하여 임베딩 벡터 생성
- **은닉층**:
  - 크기는 하이퍼파라미터 (예: 100 뉴런)
  - 입력층의 90개 뉴런과 완전 연결
- **출력층**: 
  - 17,000개 뉴런 (각 단어의 확률)
  - 은닉층과 완전 연결
- **소프트맥스 층**: 
  - 출력층의 로짓을 확률 분포로 변환

### 8. 파라미터 최적화
- 출력층 가중치와 편향
- 은닉층 가중치와 편향
- 임베딩 룩업 테이블 C
- 모든 파라미터는 역전파를 통해 동시에 최적화됨

## (re-)building our training dataset


### 1. 초기 설정

```python
import torch
import matplotlib.pyplot as plt

# 이름 목록 읽기
words = open('names.txt', 'r').read().splitlines()
print(words[:8])  # 처음 8개 이름 출력

# 문자 집합 생성 및 매핑
chars = sorted(list(set(''.join(words))))
stoi = {s:i+1 for i,s in enumerate(chars)}
stoi['.'] = 0
itos = {i:s for s,i in stoi.items()}
print(itos)
```

- PyTorch와 Matplotlib 라이브러리를 임포트합니다.
- 'names.txt' 파일에서 이름 목록을 읽어옵니다.
- 모든 이름에서 사용된 고유 문자를 추출하고 정렬합니다.
- 문자를 정수로 매핑하는 딕셔너리(stoi)와 그 역(itos)을 생성합니다.
- '.' 문자를 패딩으로 사용하며, 이를 0으로 매핑합니다.

### 2. 데이터셋 생성 함수

```python
# 데이터셋 구축

# 컨텍스트 길이 설정: 다음 문자를 예측하기 위해 사용할 이전 문자의 수
block_size = 3

# 입력(X)과 레이블(Y)을 저장할 리스트 초기화
X, Y = [], []

# 모든 단어에 대해 반복
for w in words:
    # 각 단어에 대한 초기 컨텍스트를 0(패딩)으로 설정
    context = [0] * block_size
    
    # 단어의 각 문자와 단어 끝을 나타내는 '.'에 대해 반복
    for ch in w + '.':
        # 현재 문자의 정수 인덱스를 가져옴
        ix = stoi[ch]
        
        # 현재 컨텍스트를 입력(X)에 추가
        X.append(context)
        
        # 현재 문자의 인덱스를 레이블(Y)에 추가
        Y.append(ix)
        
        #컨텍스트 업데이트: 가장 왼쪽(오래된) 문자를 제거하고 새 문자를 오른쪽에 추가
        context = context[1:] + [ix]

# 입력(X)을 PyTorch 텐서로 변환
X = torch.tensor(X)

# 레이블(Y)을 PyTorch 텐서로 변환
Y = torch.tensor(Y)
```

이 코드는 다음과 같은 작업을 수행합니다:

1. `block_size`로 컨텍스트 길이를 정의합니다. 여기서는 3으로 설정되어 있어, 3개의 이전 문자를 사용해 다음 문자를 예측합니다.

2. 모든 단어에 대해 반복하면서:
   - 각 단어의 시작에서 컨텍스트를 패딩(0)으로 초기화합니다.
   - 단어의 각 문자(그리고 단어의 끝을 나타내는 '.')에 대해:
     - 현재 문자의 정수 인덱스를 가져옵니다.
     - 현재 컨텍스트를 X에 추가합니다.
     - 현재 문자의 인덱스를 Y에 추가합니다.
     - 컨텍스트를 업데이트합니다 (가장 오래된 문자 제거, 새 문자 추가).

3. 최종적으로 X와 Y를 PyTorch 텐서로 변환합니다.

### 4. 데이터셋 특성
- 각 입력(X)은 3개의 정수로 구성됩니다 (block_size가 3이므로).
- 각 레이블(Y)은 예측해야 할 다음 문자의 정수 인덱스입니다.
- 패딩('.')은 0으로 표현됩니다.

### 5. 블록 크기 변경
- `block_size`를 변경하여 컨텍스트 길이를 조절할 수 있습니다.
- 예: `block_size = 4`로 설정하면 4개 문자로 5번째 문자를 예측합니다.
- `block_size = 10`으로 설정하면 10개 문자로 11번째 문자를 예측합니다.

### 6. 최종 데이터셋
- 5개 단어에서 32개의 예제가 생성됩니다.
- X: (32, 3) 형태의 텐서 (32개 예제, 각 3개의 정수)
- Y: (32,) 형태의 텐서 (32개의 레이블)


## implementing the embedding lookup table


### 1. 임베딩 테이블 초기화

```python
import torch

# 가능한 문자 수 (26개 알파벳 + '.')
num_chars = 27
# 임베딩 차원
embedding_dim = 2

# 임베딩 룩업 테이블을 랜덤하게 초기화
C = torch.randn((num_chars, embedding_dim))

print(C.shape, C)
```

### 2. 단일 정수 임베딩

```python
# 단일 정수 임베딩 (예: 5)
idx = 5
print(C[idx])
```

### 3. 원-핫 인코딩을 사용한 임베딩 (참고용)

```python
# 원-핫 인코딩 방식 (참고용)
one_hot = torch.nn.functional.one_hot(torch.tensor(5), num_classes=27).float() #float만 @연산 가능 int일 경우 안됨
print(one_hot @ C)
```

### 4. 다중 정수 동시 임베딩

```python
# 다중 정수 동시 임베딩
idx_list = [5, 6, 7]
print(C[idx_list])

# 텐서를 사용한 인덱싱
idx_tensor = torch.tensor([5, 6, 7])
print(C[idx_tensor])

# 반복 인덱싱
idx_repeat = torch.tensor([7, 7, 7])
print(C[idx_repeat])
```

### 5. 다차원 텐서 임베딩

```python
# 2차원 텐서 임베딩
X = torch.randint(0, 27, (32, 3))  # (32, 3) 크기의 랜덤 정수 텐서
emb = C[X]  # (32, 3, 2) 크기의 임베딩 결과

print(emb.shape)
print(emb[13, 2])  # 13번째 예제의 3번째 문자 임베딩
print(C[X[13, 2]])  # 동일한 결과 확인
```

### 주요 설명 포인트:

1. `C`는 (27, 2) 형태의 임베딩 룩업 테이블입니다.
2. 단일 정수 임베딩은 간단히 `C[idx]`로 수행됩니다.
3. 원-핫 인코딩 방식은 참고용으로 제시되었으며, 실제로는 사용하지 않습니다.
4. PyTorch의 유연한 인덱싱 기능을 사용하여 리스트, 텐서, 다차원 텐서로 동시에 여러 임베딩을 검색할 수 있습니다.
5. `X`가 (32, 3) 형태의 입력 텐서일 때, `C[X]`는 (32, 3, 2) 형태의 임베딩 결과를 생성합니다.

## implementing the hidden layer + internals of torch.Tensor: storage, views

### 1. 은닉층 구현

```python
# 임베딩 룩업 테이블 C와 입력 X가 이미 정의되어 있다고 가정
emb = C[X]  # (32, 3, 2) 형태의 임베딩 결과

# 은닉층 가중치와 편향 초기화
W1 = torch.randn((6, 100))  # 6 = 3 * 2 (block_size * embedding_dim)
b1 = torch.randn(100)

# 입력 재구성 방법 1: torch.cat 사용 (비효율적)
# x = torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], dim=1)

# 입력 재구성 방법 2: torch.unbind 사용
# x = torch.cat(torch.unbind(emb, dim=1), dim=1)

# 입력 재구성 방법 3: view 사용 (가장 효율적)
x = emb.view(emb.shape[0], -1)  # (32, 6) 형태로 변환

# 은닉층 계산
h = torch.tanh(x @ W1 + b1)  # (32, 100) 형태의 은닉층 출력

print(h.shape)  # 출력: torch.Size([32, 100])
```

### 2. torch.Tensor의 내부 구조: Storage와 Views

```python
# 1차원 텐서 생성
a = torch.arange(18)
print(a.shape)  # 출력: torch.Size([18])

# 다양한 view 생성
b = a.view(2, 9)
c = a.view(9, 2)
d = a.view(3, 3, 2)

print(b.shape)  # 출력: torch.Size([2, 9])
print(c.shape)  # 출력: torch.Size([9, 2])
print(d.shape)  # 출력: torch.Size([3, 3, 2])

# 모든 view가 동일한 데이터를 공유함을 확인
a[0] = 100
print(b[0, 0])  # 출력: 100
print(c[0, 0])  # 출력: 100
print(d[0, 0, 0])  # 출력: 100
```

### 주요 포인트 및 설명:

1. **입력 재구성 방법들**:
   - `torch.cat`: 각 임베딩을 개별적으로 추출하고 연결합니다. 새로운 메모리를 할당하므로 비효율적입니다.
   - `torch.unbind`: 텐서를 지정된 차원을 따라 분리합니다. `cat`보다 더 일반적이지만 여전히 새 메모리를 할당합니다.
   - `view`: 가장 효율적인 방법. 새로운 메모리 할당 없이 텐서의 형상을 변경합니다.

2. **torch.Tensor의 내부 구조**:
   - 텐서의 실제 데이터는 1차원 배열인 'storage'에 저장됩니다.
   - `view` 메서드는 storage offset, strides, shapes를 조작하여 텐서의 형상을 변경합니다.
   - 다양한 view들은 동일한 underlying storage를 공유합니다.

3. **브로드캐스팅**:
   - `x @ W1 + b1`에서 b1은 자동으로 모든 배치 예제에 대해 브로드캐스팅됩니다.
   - 브로드캐스팅은 오른쪽에서 왼쪽으로 정렬되며, 필요한 경우 새로운 차원이 추가됩니다.

4. **효율성**:
   - `view`를 사용한 재구성은 메모리 효율적입니다.
   - `torch.cat`과 같은 연산은 새로운 메모리를 할당하므로 덜 효율적입니다.

5. **유연성**:
   - `emb.view(emb.shape, -1)`에서 `-1`을 사용하여 PyTorch가 자동으로 차원을 추론하도록 할 수 있습니다.


## implementing the output layer

### 1. 출력층 가중치와 편향 초기화
```python
W2 = torch.randn((100, 27))  # 100: 은닉층 크기, 27: 가능한 문자 수
b2 = torch.randn(27)
```
- 은닉층의 출력(100)을 27개의 가능한 문자로 매핑합니다.

### 2. 로짓 계산
```python
logits = h @ W2 + b2  # 형상: (32, 27)
```
- `h`: 은닉층의 출력 (32, 100)
- 행렬 곱셈 후 편향을 더해 로짓을 계산합니다.

### 3. 확률 계산 (소프트맥스 구현)
```python
counts = torch.exp(logits)
probs = counts / counts.sum(dim=1, keepdim=True)
```
- 이전 강의의 비그램 모델과 동일한 방식으로 소프트맥스를 구현합니다.
- `exp`로 로짓을 지수화하고, 각 행별로 정규화하여 확률 분포를 생성합니다.

### 4. 형상 및 정규화 확인
```python
print(logits.shape)  # 출력: torch.Size([32, 27])
print(probs.shape)   # 출력: torch.Size([32, 27])
print(probs.sum(dim=1))  # 각 행의 합이 1인지 확인
```
- 로짓과 확률의 형상이 예상대로인지 확인합니다.
- 각 행의 합이 1인지 확인하여 올바른 확률 분포인지 검증합니다.

### 주요 포인트:
1. 출력층은 은닉층의 출력을 받아 각 문자의 확률을 계산합니다.
2. 소프트맥스 함수는 로짓을 확률로 변환합니다.
3. PyTorch의 브로드캐스팅 기능이 편향 더하기에 자동으로 적용됩니다.
4. 이 구현은 이전 강의의 비그램 모델을 확장한 것입니다.



## implementing the negative log likelihood loss

```python
# 로짓을 지수화하여 counts 얻기
counts = torch.exp(logits)

# counts를 정규화하여 확률 분포 얻기
# dim=1: 각 행(예제)에 대해 정규화
# keepdim=True: 결과의 차원을 유지
probs = counts / counts.sum(dim=1, keepdim=True)

# 확률 분포의 형태 확인
print(probs.shape)  # 출력: torch.Size([32, 27])

# 각 행의 합이 1인지 확인 (정규화 검증)
print(probs.sum(dim=1))

# 정답 문자의 확률 추출
# torch.arange(32): 0부터 31까지의 배치 인덱스
# Y: 각 예제의 정답 문자 인덱스
correct_probs = probs[torch.arange(32), Y]

# 추출된 확률 출력
print(correct_probs)

# Negative Log Likelihood 손실 계산
# torch.log(): 자연로그 계산
# .mean(): 평균 계산
loss = -torch.log(correct_probs).mean()

# 최종 손실 출력
print(f"Loss: {loss.item():.4f}")
```

주석 설명:

1. `counts = torch.exp(logits)`: 로짓을 지수화하여 'fake counts'를 생성합니다. 이는 소프트맥스 함수의 첫 단계입니다.

2. `probs = counts / counts.sum(dim=1, keepdim=True)`: 
   - 각 예제(행)에 대해 counts를 정규화하여 확률 분포를 얻습니다.
   - `dim=1`은 각 행에 대해 연산을 수행함을 의미합니다.
   - `keepdim=True`는 결과의 차원을 유지하여 브로드캐스팅이 올바르게 작동하도록 합니다.

3. `print(probs.shape)`: 확률 분포의 형태를 확인합니다. [32, 27]은 32개의 예제 각각에 대해 27개 문자의 확률을 나타냅니다.

4. `print(probs.sum(dim=1))`: 각 예제(행)의 확률 합이 1인지 확인하여 올바르게 정규화되었는지 검증합니다.

5. `correct_probs = probs[torch.arange(32), Y]`:
   - 각 예제에 대해 정답 문자의 예측 확률을 추출합니다.
   - `torch.arange(32)`는 배치 내 각 예제의 인덱스(0부터 31까지)를 생성합니다.
   - `Y`는 각 예제의 정답 문자 인덱스입니다.

6. `loss = -torch.log(correct_probs).mean()`:
   - 정답 확률의 로그를 취하고, 그 평균의 음수를 계산하여 손실을 구합니다.
   - 이는 Negative Log Likelihood 손실 함수의 구현입니다.

7. `print(f"Loss: {loss.item():.4f}")`: 계산된 손실 값을 소수점 4자리까지 출력합니다.

이 구현은 신경망의 예측을 평가하고, 학습 신호를 제공하는 핵심 부분입니다. 손실 값이 낮을수록 모델의 예측이 정확함을 의미합니다.




## Summary of the Full Network

```python
# 데이터셋 형태 확인
print(Xtr.shape, Ytr.shape)  # 데이터셋 크기 출력

# 재현성을 위한 랜덤 시드 설정
g = torch.Generator().manual_seed(2147483647)

# 파라미터 초기화
C = torch.randn((27, 10), generator=g)  # 문자 임베딩
W1 = torch.randn((30, 200), generator=g)  # 첫 번째 가중치 행렬
b1 = torch.randn(200, generator=g)  # 첫 번째 편향
W2 = torch.randn((200, 27), generator=g)  # 두 번째 가중치 행렬
b2 = torch.randn(27, generator=g)  # 두 번째 편향

# 모든 파라미터를 하나의 리스트로 모음
parameters = [C, W1, b1, W2, b2]

# 총 파라미터 수 계산
print(sum(p.nelement() for p in parameters))  # 총 파라미터 수 출력

# Forward pass
emb = C[X]  # (32, 3, 10)
hidden = torch.tanh(emb.view(-1, 30) @ W1 + b1)  # (32, 200)
logits = hidden @ W2 + b2  # (32, 27)
#counts = logits.exp()
#prob = counts/ counts.sum(1,keepdims = True)
#loss = -prob(torch.arrange(32,Y)).log().mean()
loss = F.cross_entropy(logits, Y)
```

주요 포인트:
1. `Xtr.shape, Ytr.shape`를 출력하여 데이터셋의 크기를 확인합니다.
2. `torch.Generator().manual_seed(2147483647)`를 사용하여 재현 가능한 결과를 위해 랜덤 시드를 설정합니다.
3. 각 파라미터(C, W1, b1, W2, b2)를 랜덤으로 초기화합니다.
4. `parameters` 리스트에 모든 파라미터를 모아 관리합니다.
5. `sum(p.nelement() for p in parameters)`로 총 파라미터 수를 계산합니다.
6. Forward pass는 이전과 동일하게 구현됩니다.

이 코드는 신경망의 초기 설정과 파라미터 초기화를 보여줍니다. 재현성을 위해 랜덤 시드를 사용하고, 각 파라미터의 크기와 초기화 방법을 명확히 보여줍니다. 또한 총 파라미터 수를 계산하여 모델의 복잡도를 파악할 수 있게 합니다.

## Introducing F.cross_entropy and Why


```python
# 이전 구현 (교육 목적)
counts = torch.exp(logits)
probs = counts / counts.sum(1, keepdim=True)
loss = -torch.log(probs[torch.arange(32), Y]).mean()

# F.cross_entropy 사용 (실제 사용)
loss = F.cross_entropy(logits, Y)
```

`F.cross_entropy` 사용 이유:

1. 효율성:
   - 중간 텐서(counts, probs)를 생성하지 않아 메모리 효율적입니다.
   - PyTorch가 연산을 최적화된 커널로 융합하여 실행 속도가 빠릅니다.

2. 수치적 안정성:
   - 내부적으로 로짓의 최대값을 빼서 오버플로우를 방지합니다.
   ```python
   # 예: 극단적인 로짓 값
   logits = torch.tensor([-100, 100])
   # F.cross_entropy는 내부적으로 최대값(100)을 빼서 계산
   # 결과: tensor([0, -200])  # 안정적인 계산
   ```

3. 역전파 효율성:
   - 수학적으로 단순화된 그래디언트 계산으로 역전파가 더 효율적입니다.

4. 코드 간결성:
   - 한 줄로 손실 계산이 가능하여 코드가 간결해집니다.

주의사항:
- `F.cross_entropy`는 로짓을 입력으로 받으며, 내부적으로 소프트맥스를 적용합니다.
- 타겟(Y)은 정수 인덱스 형태여야 합니다.

이 정리는 전체 신경망 구조의 개요와 `F.cross_entropy` 함수 사용의 이점을 설명합니다. 코드와 설명을 통해 실제 구현 방식과 그 이유를 이해할 수 있도록 구성했습니다.

33분부터 다시 
## implementing the training loop, overfitting one batch

## training on the full dataset, minibatches

## finding a good initial learning rate

## splitting up the dataset into train/val/test splits and why

## experiment: larger hidden layer

## title="visualizing the character embeddings"

## experiment: larger embedding size

## summary of our final code, conclusion

## sampling from the model

## google collab (new!!) notebook advertisement

