---
layout: post
title:  "Let's build GPT: from scratch, in code, spelled out."
date:   2024-10-01 12:00:00 +0900
categories: [LLM]
tags: [DeepLearning , LLM, GPT,Transformer ]
math: true
---

# Let's build GPT: from scratch, in code, spelled out.

## URL 
https://www.youtube.com/watch?v=kCc8FmEb1nY

https://github.com/karpathy/nanoGPT

## intro: ChatGPT , Transformers , nanoGPT,Shakespeare


## ChatGPT 소개

ChatGPT는 최근 AI 커뮤니티에서 큰 주목을 받고 있는 대화형 AI 시스템입니다. 이 시스템은 사용자가 텍스트 기반 작업을 요청하면 AI가 이를 수행하는 방식으로 작동합니다.

주요 특징:
1. 텍스트 기반 작업 수행: 사용자는 ChatGPT에게 다양한 텍스트 기반 작업을 요청할 수 있습니다.
2. 확률적 시스템: 동일한 프롬프트에 대해 매번 약간씩 다른 응답을 생성할 수 있습니다.
3. 순차적 생성: 왼쪽에서 오른쪽으로 단어를 순차적으로 생성합니다.

예시:
- AI의 중요성에 대한 짧은 시 작성 요청:
  결과 1: "AI knowledge brings prosperity for all to see, Embrace its power"
  결과 2: "AI's power to grow, ignorance holds us back, learn Prosperity waits"

ChatGPT의 다양한 활용 예:
- "개에게 설명하듯이 HTML 설명하기"
- "체스 2의 릴리스 노트 작성하기"
- "일론 머스크의 트위터 인수에 대한 메모 작성하기"
- "나무에서 떨어지는 잎에 대한 속보 기사 작성하기"

## Transformers 아키텍처

Transformer는 ChatGPT의 핵심 기술로, 2017년 "Attention is All You Need" 논문에서 처음 소개되었습니다.

주요 특징:
1. 랜드마크 논문: AI 분야에 큰 영향을 미친 중요한 논문입니다.
2. 원래 목적: 기계 번역을 위해 개발되었으나, 이후 AI의 다양한 분야에 적용되었습니다.
3. 광범위한 적용: minor 변경을 거쳐 다양한 AI 응용 분야에 사용되고 있습니다.
4. ChatGPT의 핵심: ChatGPT의 핵심 기술로 사용되고 있습니다.

GPT (Generatively Pre-trained Transformer):
- GPT는 "Generatively Pre-trained Transformer"의 약자입니다.
- Transformer 아키텍처를 기반으로 하는 언어 모델입니다.

## nanoGPT


주요 특징:
1. 간단한 구현: 두 개의 파일로 구성되어 있으며, 각 파일은 300줄의 코드로 이루어져 있습니다.
2. 구성:
   - 하나의 파일은 GPT 모델(Transformer)을 정의합니다.
   - 다른 파일은 주어진 텍스트 데이터셋에 대해 모델을 훈련시킵니다.
3. 성능: Open WebText 데이터셋으로 훈련시키면 GPT-2의 성능을 재현할 수 있습니다.
4. 목적: Transformer의 작동 원리를 이해하고 직접 구현해볼 수 있도록 돕습니다.

## Shakespeare 데이터셋

강의에서 사용되는 "tiny Shakespeare" 데이터셋의 특징:

1. 구성: Shakespeare의 모든 작품을 하나의 파일로 연결한 것입니다.
2. 크기: 약 1MB 크기의 파일입니다.
3. 목적: 문자들이 어떻게 서로 뒤따르는지 모델링하는 데 사용됩니다.
4. 작동 방식: 
   - 주어진 문맥의 문자들을 보고, Transformer 신경망이 다음에 올 문자를 예측합니다.
   - 이 과정을 통해 데이터 내의 모든 패턴을 모델링하게 됩니다.

훈련 결과:
- 시스템 훈련 후, "Shakespeare 스타일"의 무한한 텍스트를 생성할 수 있습니다.
- 생성된 텍스트는 실제 Shakespeare가 아닌, Shakespeare처럼 보이는 가짜 텍스트입니다.
- 문자 단위로 생성됩니다 (ChatGPT는 토큰 단위로 생성).


## reading and exploring the data

``` python
# tiny shakespeare 데이터셋을 다운
!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
```

``` python
#  파일을 읽어 내용물을 확인 
with open('input.txt', 'r', encoding='utf-8') as f:
    text = f.read()
```

``` python
print("length of dataset in characters: ", len(text))
# length of dataset in characters:  1115394

# let's look at the first 1000 characters
print(text[:1000])
```

```python

# here are all the unique characters that occur in this text
chars = sorted(list(set(text)))
vocab_size = len(chars)
print(''.join(chars))
print(vocab_size)

> !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
>65 #총 문자는 65개 
```

## tokenization , train/val split 

### Tokenization

토큰화(tokenization)는 원시 텍스트 문자열을 정수 시퀀스로 변환하는 과정입니다. 이는 텍스트를 모델이 이해할 수 있는 형태로 변환하는 중요한 단계입니다.

1. 문자 수준 토큰화:
   - 이 강의에서는 문자 수준의 언어 모델을 구축하므로, 개별 문자를 정수로 변환합니다.
   - 예: "hi there"를 [46, 47, ...]와 같은 정수 리스트로 변환합니다.

```python
# 문자를 정수로, 정수를 문자로 변환하는 딕셔너리 생성

stoi = { ch:i for i,ch in enumerate(chars) }
itos = { i:ch for i,ch in enumerate(chars) }

# 인코딩 함수: 문자열을 정수 리스트로 변환
def encode(s):
    return [stoi[c] for c in s] 

# 디코딩 함수: 정수 리스트를 문자열로 변환
def decode(l):
    return ''.join([itos[i] for i in l])
```

이 코드는 다음과 같은 작업을 수행합니다:
- 텍스트에서 고유한 문자들을 추출하고 정렬합니다.
- 각 문자에 고유한 정수를 할당합니다 (stoi).
- 각 정수에 해당하는 문자를 저장합니다 (itos).
- encode 함수는 문자열을 정수 리스트로 변환합니다.
- decode 함수는 정수 리스트를 원래 문자열로 복원합니다.

2. 토큰화 예시:

```python
text = "hi there"
encoded = encode(text)
print(encoded)  # 예: [46, 47, 1, 58, 46, 43, 56, 43]
decoded = decode(encoded)
print(decoded)  # "hi there"
```

3. 전체 데이터셋 토큰화:

```python
import torch

data = torch.tensor(encode(text), dtype=torch.long)
print(data.shape, data.dtype)
print(data[:1000])  # 처음 1000개 토큰 출력
```
이 코드는 전체 Shakespeare 텍스트를 토큰화하고 PyTorch 텐서로 변환합니다. 결과는 긴 1차원 정수 시퀀스가 됩니다.


2. 인코더와 디코더:
   - 인코더: 문자열을 정수 리스트로 변환합니다.
   - 디코더: 정수 리스트를 원래 문자열로 복원합니다.
   - 이를 위해 문자와 정수 간의 양방향 매핑 테이블을 생성합니다.

3. 구현 방법:
   - 모든 문자를 반복하며 문자-정수 간 룩업 테이블을 생성합니다.
   - 인코딩: 각 문자를 개별적으로 정수로 변환합니다.
   - 디코딩: 역매핑을 사용하여 정수를 문자로 변환하고 연결합니다.

4. 다른 토큰화 방식:
   - 문장 조각(Sentence Piece): Google에서 사용하는 서브워드 수준 토크나이저입니다.
   - Byte Pair Encoding: OpenAI의 GPT에서 사용하는 방식으로, tiktoken 라이브러리로 구현됩니다.
   - 단어 수준 토큰화: 전체 단어를 정수로 인코딩하는 방식입니다.

5. 토큰화 방식의 트레이드오프:
   - 작은 어휘 크기와 긴 시퀀스 vs 큰 어휘 크기와 짧은 시퀀스
   - 예: 문자 수준(65개 토큰)과 GPT-2(50,000개 토큰)의 차이

6. 강의에서의 선택:
   - 간단한 구현을 위해 문자 수준 토큰화를 사용합니다.
   - 작은 코드북 크기, 간단한 인코딩/디코딩 함수를 가지지만, 매우 긴 시퀀스가 생성됩니다.

### Train/Val Split

훈련 데이터와 검증 데이터를 분리하는 것은 모델의 성능을 평가하고 과적합을 방지하는 데 중요합니다.

1. 데이터 준비:
   - PyTorch 라이브러리의 torch.tensor를 사용하여 전체 Shakespeare 텍스트를 토큰화합니다.
   - 결과는 정수의 긴 시퀀스로, 원본 텍스트의 문자를 나타냅니다.

2. 분할 비율:
   - 훈련 데이터: 전체 데이터셋의 처음 90%
   - 검증 데이터: 마지막 10%

3. 분할의 목적:
   - 과적합 정도를 이해하고 평가하기 위함입니다.
   - 모델이 단순히 Shakespeare 텍스트를 완벽히 암기하는 것이 아니라, Shakespeare 스타일의 텍스트를 생성할 수 있도록 하는 것이 목표입니다.

4. 검증 데이터의 역할:
   - 모델이 학습하지 않은 데이터에 대한 성능을 평가합니다.
   - 실제 Shakespeare 텍스트와 유사한 텍스트를 생성할 수 있는지 확인합니다.

5. 구현:
   - 전체 데이터셋을 torch.tensor로 변환한 후, 인덱싱을 사용하여 훈련 데이터와 검증 데이터로 분할합니다.

    1. 데이터 분할:

    ```python
    # 데이터를 훈련 세트(90%)와 검증 세트(10%)로 분할
    n = int(0.9 * len(data))
    train_data = data[:n]
    val_data = data[n:]
    ```

    이 코드는 다음과 같은 작업을 수행합니다:
    - 전체 데이터의 90%를 훈련 데이터로 사용합니다.
    - 나머지 10%를 검증 데이터로 사용합니다.

    2. 분할의 목적:
    - 과적합 정도를 평가합니다.
    - 모델이 단순히 Shakespeare 텍스트를 암기하는 것이 아니라, Shakespeare 스타일의 텍스트를 생성할 수 있도록 합니다.


## Data Loader: Batches of Chunks of Data

1. 데이터 입력 방식

Transformer 모델은 전체 텍스트를 한 번에 처리하지 않습니다. 대신, 텍스트의 작은 청크(chunk)들을 사용하여 학습합니다. 이는 계산 비용을 줄이고 효율성을 높이기 위함입니다.

2. 블록 크기 (Block Size)

- 정의: 한 번에 처리할 텍스트의 최대 길이
- 코드에서는 주로 'block_size'로 표현됨
- 다른 용어로는 'context length'라고도 불림
- 예시 코드에서는 block_size = 8로 설정

3. 데이터 청크 예시

```python
block_size = 8
x = train_data[:block_size]
y = train_data[1:block_size+1]
for t in range(block_size):
    context = x[:t+1]
    target = y[t]
    print(f"when input is {context} the target: {target}")
```

```
>
when input is tensor([18]) the target: 47
when input is tensor([18, 47]) the target: 56
when input is tensor([18, 47, 56]) the target: 57
when input is tensor([18, 47, 56, 57]) the target: 58
when input is tensor([18, 47, 56, 57, 58]) the target: 1
when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15
when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47
when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58
```

- x는 입력 시퀀스
- y는 목표 시퀀스 (x에서 한 칸 이동)
- 각 단계에서 context는 증가하고, 그에 따른 target이 있음

4. 배치 차원 (Batch Dimension)

여러 청크를 동시에 처리하기 위해 배치 차원을 추가합니다. 이는 GPU 활용을 최적화하기 위함입니다.

```python
import torch
torch.manual_seed(1337)
batch_size = 4
block_size = 8

def get_batch(split):
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    return x, y

xb, yb = get_batch('train')
```

이 코드의 주요 특징:
- batch_size: 한 번에 처리할 독립적인 시퀀스의 수 (여기서는 4)
- block_size: 최대 컨텍스트 길이 
- get_batch 함수: 
  - 무작위로 데이터에서 위치를 선택
  - 선택된 위치에서 block_size 길이의 청크를 추출
  - x는 입력 시퀀스, y는 목표 시퀀스 (x에서 한 칸 이동)
- 결과: xb는 (4, 8) 형태의 텐서, yb도 같은 형태 4rows 8col (8col-> chunck of data set )

5. 배치 내 예제 설명

```python
for b in range(batch_size):
    for t in range(block_size):
        context = xb[b, :t+1]
        target = yb[b,t]
        print(f"when input is {context.tolist()} the target: {target}")
```

이 코드는 배치 내의 모든 예제를 보여줍니다:
- 4x8 배열에는 총 32개의 독립적인 예제가 포함됨
- 각 행은 서로 독립적인 시퀀스
- 각 위치에서 입력(context)과 그에 따른 목표(target)가 있음

6. Transformer 입력 준비

이렇게 준비된 xb 텐서가 Transformer에 입력됩니다. Transformer는 이 모든 예제를 동시에 처리하고, 각 위치에서 다음 문자를 예측하려고 시도합니다.

7. 학습의 이점

- 다양한 길이의 컨텍스트로 학습: 1부터 block_size까지
- 추론 시 유연성: 모델은 짧은 컨텍스트부터 긴 컨텍스트까지 모두 처리 가능
- 효율성: 병렬 처리를 통해 GPU 활용도 극대화

이렇게 준비된 데이터는 Transformer 모델에 입력되어 학습을 진행하게 됩니다. 이 방식은 대규모 텍스트 데이터를 효율적으로 처리하면서도, 다양한 길이의 컨텍스트에 대해 모델을 훈련시킬 수 있게 해줍니다.



## simplest baseline: bigram language model, loss, generation


## training the bigram model


## port our code to a script


## version 1: averaging the past context with for loops , the weakest form of aggregation 


## the trick in self-attention: matrix multiply as weighted aggregation

