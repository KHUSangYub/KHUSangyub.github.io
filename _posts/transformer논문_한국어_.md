
3page

**모델 아키텍처**

대부분의 경쟁력 있는 신경망 기반 시퀀스 변환 모델들은 인코더-디코더 구조를 가지고 있습니다. 여기서 인코더는 기호 표현의 입력 시퀀스 (x₁, ..., xₙ)를 연속적인 표현의 시퀀스 z = (z₁, ..., zₙ)로 매핑합니다. z를 기반으로, 디코더는 한 번에 하나의 요소씩 기호의 출력 시퀀스 (y₁, ..., yₘ)를 생성합니다. 각 단계에서 모델은 오토레그레시브(autogressive) 방식으로 작동하며, 다음을 생성할 때 이전에 생성된 기호들을 추가 입력으로 사용합니다.

Transformer는 이 기본 구조를 따르며, 인코더와 디코더 모두에 대해 중첩된 자기-어텐션(self-attention)과 위치별(point-wise) 완전 연결된 계층을 사용합니다. 이는 각각 그림 1의 좌측과 우측 절반에 나타나 있습니다.

### 3.1 인코더와 디코더 스택

- **인코더**: 인코더는 N = 6개의 동일한 계층으로 구성된 스택입니다. 각 계층은 두 개의 서브 계층을 가지고 있습니다. 첫 번째는 멀티-헤드 자기-어텐션 메커니즘이고, 두 번째는 간단한 위치별 완전 연결된 피드-포워드 네트워크입니다. 우리는 두 서브 계층 각각에 대해 레지듀얼 연결(residual connection)을 사용하고, 그 후 계층 정규화를 수행합니다.

**디코더**: 디코더도 N = 6개의 동일한 계층으로 구성된 스택입니다. 각 인코더 계층의 두 서브 계층 외에도, 디코더는 인코더 스택의 출력에 대해 멀티-헤드 어텐션을 수행하는 세 번째 서브 계층을 추가합니다.

----
4page

**3.2 어텐션(Attention)**

어텐션 함수는 쿼리와 일련의 키-값 쌍을 출력으로 매핑하는 함수로 설명될 수 있습니다. 여기서 쿼리, 키, 값, 출력은 모두 벡터로 표현됩니다. 출력은 값의 가중합으로 계산되며, 각 값에 할당된 가중치는 쿼리와 해당 키의 호환성 함수에 의해 결정됩니다.

### 3.2.1 스케일 조정된 점곱 어텐션(Scaled Dot-Product Attention)

이 논문에서는 "스케일 조정된 점곱 어텐션"이라는 특정 어텐션 메커니즘을 사용합니다 (그림 2 참고). 입력은 `dk` 차원의 쿼리와 키, 그리고 `dv` 차원의 값을 포함합니다. 우리는 쿼리와 모든 키 간의 점곱을 계산하고, 각 점곱을 `√dk`로 나눈 후 소프트맥스(softmax) 함수를 적용하여 값에 대한 가중치를 얻습니다.

실제로 우리는 여러 쿼리 집합에 대해 동시에 어텐션 함수를 계산하며, 쿼리, 키, 값은 각각 행렬 `Q`, `K`, `V`로 함께 묶습니다. 그런 다음, 출력 행렬을 다음과 같이 계산합니다:

\[
Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{dk}}\right)V
\]

가장 일반적으로 사용되는 두 가지 어텐션 함수는 덧셈 어텐션(additive attention)과 점곱 어텐션(dot-product attention)입니다. 점곱 어텐션은 스케일링 요소 `1/√dk`를 제외하면 이 논문의 알고리즘과 동일합니다. 덧셈 어텐션은 단일 은닉층을 가진 피드포워드 네트워크를 사용하여 호환성 함수를 계산합니다. 이 두 가지 메커니즘은 이론적으로 복잡도가 비슷하지만, 실제로는 점곱 어텐션이 매우 최적화된 행렬 곱셈 코드로 구현할 수 있어 훨씬 빠르고 공간 효율적입니다.

`dk`의 값이 작을 때는 두 메커니즘의 성능이 비슷하지만, `dk`가 클 경우 덧셈 어텐션이 스케일링 없는 점곱 어텐션보다 더 나은 성능을 보입니다. 우리는 `dk`가 클수록 점곱이 크기가 커져 소프트맥스 함수가 매우 작은 그래디언트가 있는 영역으로 밀려난다고 추측합니다. 이 문제를 해결하기 위해 우리는 점곱을 `1/√dk`로 스케일링합니다.

---
5page

다음은 **3.2.2 Multi-Head Attention** 부분의 한국어 번역입니다:

---

### 3.2.2 멀티-헤드 어텐션 (Multi-Head Attention)

단일 어텐션 함수를 \(d_{model}\)-차원의 키(key), 값(value), 쿼리(query)로 수행하는 대신, 우리는 쿼리, 키, 값을 각각 \(d_k\), \(d_k\), \(d_v\) 차원으로 선형 투영(linear projection)하는 서로 다른 학습된 선형 투영을 사용하여 \(h\)번 수행하는 것이 유익하다는 것을 발견했습니다. 이렇게 투영된 쿼리, 키, 값 각각에 대해 병렬로 어텐션 기능을 수행하여 \(d_v\)-차원의 출력 값을 생성합니다. 그런 다음 이들을 연결하여 다시 한 번 투영을 수행해 최종 출력을 생성합니다. 이는 **그림 2**에 나와 있습니다.

멀티-헤드 어텐션은 모델이 서로 다른 위치에서 서로 다른 표현 하위 공간으로부터 정보를 동시에 참조할 수 있도록 합니다. 단일 어텐션 헤드에서는 평균화가 이런 정보를 방해합니다.

\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W_O
\]

여기서 \(\text{head}_i = \text{Attention}(QW_{Q_i}, KW_{K_i}, VW_{V_i})\)입니다.

여기서 투영 행렬들은 각각 \(W_{Q_i} \in \mathbb{R}^{d_{model} \times d_k}\), \(W_{K_i} \in \mathbb{R}^{d_{model} \times d_k}\), \(W_{V_i} \in \mathbb{R}^{d_{model} \times d_v}\), 그리고 \(W_O \in \mathbb{R}^{hd_v \times d_{model}}\)입니다.

이 논문에서는 \(h = 8\)개의 병렬 어텐션 계층 또는 헤드를 사용합니다. 각 헤드에 대해 \(d_k = d_v = d_{model} / h = 64\)를 사용합니다. 각 헤드의 차원이 줄어들기 때문에, 전체 계산 비용은 전체 차원의 단일 헤드 어텐션과 유사합니다.

--- 
6page

3.2.3 **모델에서 어텐션의 적용**

Transformer는 세 가지 다른 방식으로 멀티-헤드 어텐션을 사용합니다:

- **인코더-디코더 어텐션** 계층에서는 쿼리가 이전 디코더 계층에서 나오고, 메모리 키와 값은 인코더의 출력에서 나옵니다. 이를 통해 디코더의 각 위치는 입력 시퀀스의 모든 위치에 어텐션을 수행할 수 있습니다. 이것은 일반적인 시퀀스-투-시퀀스 모델에서 사용되는 인코더-디코더 어텐션 메커니즘과 유사합니다.

- **인코더**에는 자기-어텐션 계층이 포함되어 있습니다. 자기-어텐션 계층에서는 모든 키, 값, 쿼리가 동일한 위치에서 나옵니다. 즉, 인코더의 이전 계층의 출력에서 나온다는 의미입니다. 인코더 내의 각 위치는 이전 계층의 모든 위치에 어텐션을 적용할 수 있습니다.

- 마찬가지로, **디코더**의 자기-어텐션 계층은 디코더의 각 위치가 현재 위치까지의 모든 위치에 어텐션을 수행할 수 있도록 합니다. 디코더에서 왼쪽으로 정보가 흘러 들어가는 것을 방지하여 오토레그레시브(auto-regressive) 특성을 유지해야 합니다. 이를 위해 스케일 조정된 점곱 어텐션에서 소프트맥스 입력 중 불법적인 연결에 해당하는 값을 마스킹(−∞로 설정)합니다.

다음은 PDF의 5페이지 전체 내용의 한국어 번역입니다:

---

### 3.3 위치별 피드포워드 네트워크 (Position-wise Feed-Forward Networks)

어텐션 서브 계층 외에도, 인코더와 디코더의 각 계층에는 위치별로 동일하게 적용되는 완전 연결된 피드포워드 네트워크가 포함되어 있습니다. 이 네트워크는 각 위치에 대해 개별적으로 동일하게 적용됩니다. 두 개의 선형 변환과 그 사이에 ReLU 활성화 함수로 구성됩니다.

\[
FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2
\]

선형 변환은 다른 위치들 간에 동일하지만, 각 계층마다 서로 다른 파라미터를 사용합니다. 또 다른 방법으로는 두 개의 컨볼루션을 커널 크기 1로 적용하는 것처럼 설명할 수 있습니다. 입력 및 출력의 차원은 \(d_{model} = 512\)이며, 내부 계층의 차원은 \(d_{ff} = 2048\)입니다.

### 3.4 임베딩과 소프트맥스 (Embeddings and Softmax)

다른 시퀀스 변환 모델들과 마찬가지로, 우리는 입력 토큰과 출력 토큰을 \(d_{model}\) 차원의 벡터로 변환하기 위해 학습된 임베딩을 사용합니다. 또한, 디코더 출력에 소프트맥스 함수를 적용하여 예측된 다음 토큰의 확률을 얻기 위해 일반적인 학습된 선형 변환을 사용합니다. 우리는 두 임베딩 계층과 프리-소프트맥스 선형 변환 사이에 동일한 가중치 행렬을 공유합니다. 이는 [30]과 유사한 방법입니다. 임베딩 계층에서는 이 가중치를 \( \sqrt{d_{model}} \)로 곱합니다.

### 3.5 위치 인코딩 (Positional Encoding)

### 3.5 위치 인코딩 (Positional Encoding)

우리 모델에는 순환(recurrence)이나 합성곱(convolution)이 없기 때문에, 모델이 시퀀스의 순서를 반영할 수 있도록 입력 시퀀스의 상대적 또는 절대적 위치에 대한 정보를 주입해야 합니다. 이를 위해, 인코더와 디코더 스택의 맨 아래에서 입력 임베딩에 위치 인코딩을 추가합니다. 위치 인코딩은 임베딩과 동일한 차원 \(d_{model}\)을 가지므로, 둘을 더할 수 있습니다.

위치 인코딩 방식에는 학습된 방식과 고정된 방식이 있으며, 우리는 다양한 주파수의 사인(sine) 함수와 코사인(cosine) 함수를 사용했습니다:

\[
PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\]
\[
PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
\]

여기서 \(pos\)는 위치를, \(i\)는 차원을 나타냅니다. 각 차원의 위치 인코딩은 사인파로 표현되며, 파장의 길이는 \(2\pi\)에서 \(10000 \cdot 2\pi\)까지 기하급수적으로 증가합니다. 우리는 이 함수가 모델이 상대적 위치를 학습하기 쉽게 만들어줄 것이라고 가정했습니다. 왜냐하면, 고정된 오프셋 \(k\)에 대해 \(PE_{pos+k}\)는 \(PE_{pos}\)의 선형 함수로 표현될 수 있기 때문입니다.

우리는 또한 학습된 위치 임베딩을 사용하는 것을 실험해보았으며, 두 방식은 거의 동일한 결과를 생성했습니다 (테이블 3의 행 (E) 참고). 우리는 사인 및 코사인 방식이 훈련 중 만난 시퀀스 길이보다 더 긴 시퀀스에 대해 모델이 추론할 수 있게 할 가능성이 높다고 가정하고 이 방식을 선택했습니다.

---
















































































### 선형 투영이란 무엇인가?

**선형 투영(linear projection)**은 주어진 벡터를 다른 차원의 공간으로 변환하는 작업입니다. 이는 주로 **선형 변환**을 통해 이루어지며, 행렬 곱셈을 사용하여 벡터를 다른 차원의 공간으로 맵핑(mapping)합니다. 이 과정에서, 원래의 벡터가 **다른 차원**이나 **다른 공간**으로 변환되어도 벡터의 본질적인 정보는 유지됩니다.

수학적으로, 선형 투영은 다음과 같이 표현됩니다:

\[
y = Wx
\]

여기서:
- \( x \)는 변환하려는 원래 벡터 (차원이 \( d_{model} \))
- \( W \)는 변환을 위한 행렬 (차원이 \( d_{model} \times d_v \) 또는 다른 값일 수 있음)
- \( y \)는 변환된 결과 벡터입니다.

**벡터화(vectorization)**와 관련해서는, 벡터를 다른 차원으로 변환하는 과정이라고 볼 수 있습니다. 벡터화는 벡터의 차원을 변경하면서도 그 구조적 정보를 유지하는 작업입니다. 선형 투영도 유사하게 원래 벡터의 정보를 다른 공간으로 변환하는 역할을 합니다.

### 예시 문제로 설명: 멀티-헤드 어텐션에서의 선형 투영

#### 문제: 멀티-헤드 어텐션에서의 계산

다음과 같은 쿼리(Q), 키(K), 값(V) 벡터가 주어졌다고 합시다:

- **쿼리(Query) 벡터 \( Q \)**: \([1, 0, 1]\)
- **키(Key) 벡터 \( K \)**: \([0, 1, 0]\)
- **값(Value) 벡터 \( V \)**: \([2, 3]\)

**목표**: 쿼리 벡터 \( Q \)를 **선형 투영**하여 차원이 2인 출력 값을 구하고, 최종 어텐션 출력을 계산하시오.

#### 풀이 과정:

1. **쿼리 벡터의 선형 투영**: 
   
   우선, 주어진 쿼리 벡터 \( Q = [1, 0, 1] \)를 선형 투영하여 차원이 2인 벡터로 변환합니다. 선형 투영은 행렬 곱셈으로 표현되므로, 투영 행렬 \( W_Q \)를 사용하여 \( Q \)를 변환합니다.

   투영 행렬 \( W_Q \)를 다음과 같이 설정합시다:

   \[
   W_Q = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 1 \end{bmatrix}
   \]

   이 행렬을 \( Q \)에 곱합니다:

   \[
   Q' = W_Q \cdot Q = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 1 & 1 \end{bmatrix} \cdot \begin{bmatrix} 1 \\ 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 1 \\ 1 \\ 2 \end{bmatrix}
   \]

   즉, 쿼리 벡터는 선형 투영을 통해 차원이 2인 벡터 \( Q' = [1, 1, 2] \)로 변환되었습니다.

2. **키 벡터의 선형 투영**:
   
   마찬가지로, 키 벡터 \( K = [0, 1, 0] \)도 동일한 방법으로 선형 투영합니다. 투영 행렬 \( W_K \)를 다음과 같이 설정합시다:

   \[
   W_K = \begin{bmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{bmatrix}
   \]

   행렬 곱셈을 수행하면:

   \[
   K' = W_K \cdot K = \begin{bmatrix} 1 & 1 \\ 0 & 1 \\ 1 & 0 \end{bmatrix} \cdot \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}
   \]

   이제 키 벡터도 투영된 결과는 \( K' = [1, 1, 1] \)입니다.

3. **값(Value) 벡터의 선형 투영**:
   
   값 벡터 \( V = [2, 3] \)는 투영되지 않고 그대로 사용됩니다.

4. **어텐션 계산**:

   투영된 쿼리와 키 간의 점곱을 계산하고, 이를 스케일링합니다.

   \[
   Q' \cdot K' = 1 \times 1 + 1 \times 1 + 2 \times 1 = 4
   \]
   
   스케일링을 적용하면, 차원 \( d_k \)가 2이므로:

   \[
   \frac{4}{\sqrt{2}} \approx 2.83
   \]

   이제 소프트맥스를 적용하여 가중치를 구합니다:

   \[
   \text{softmax}(2.83) = 1 \quad (\text{다른 키가 없기 때문에 1로 간주})
   \]

5. **최종 어텐션 출력**:

   가중치를 값 벡터에 곱하면 최종 어텐션 출력은 다음과 같습니다:

   \[
   \text{출력} = 1 \times [2, 3] = [2, 3]
   \]

### 요약:

- **선형 투영(linear projection)**은 주어진 벡터를 다른 차원 공간으로 변환하는 방식으로, 행렬 곱셈을 통해 수행됩니다. 벡터의 차원을 줄이거나 늘려서 새로운 차원으로 맵핑할 때 주로 사용됩니다.
- **벡터화**는 데이터를 벡터 형태로 변환하여 컴퓨터가 처리할 수 있게 만드는 과정입니다. 선형 투영은 벡터를 다른 차원으로 맵핑하여, 여러 정보 하위 공간에서 동시에 처리할 수 있게 하는 역할을 합니다.
- 예시 문제에서는 쿼리와 키 벡터를 선형 투영한 후, 어텐션 메커니즘을 통해 최종 값을 계산하는 과정을 설명했습니다.



3.2.3
이 내용을 쉽게 설명하자면, Transformer 모델에서 **멀티-헤드 어텐션**이 어떻게 사용되는지를 세 가지 방식으로 설명하고 있습니다. 각각의 방식을 더 쉽게 풀어서 설명해 볼게요.

### 1. 인코더-디코더 어텐션 (Encoder-Decoder Attention)
- **무슨 역할을 하나요?**: 디코더는 번역된 문장의 각 단어를 생성할 때, 원래 입력된 문장의 모든 단어를 살펴보고 어텐션을 적용합니다. 이를 통해 디코더는 문장의 어느 부분이 현재 단어를 예측하는 데 중요한지 확인할 수 있습니다.
- **어떻게 동작하나요?**: 디코더가 이전 계층에서 계산된 **쿼리(Query)**를 사용하여 인코더에서 나온 **키(Key)**와 **값(Value)**에 대해 어텐션을 적용합니다. 쉽게 말해, 디코더는 입력 문장의 모든 단어를 스캔하면서 어느 단어가 중요한지 판단하고, 이를 바탕으로 다음 단어를 생성하는 겁니다.
- **예시**: 예를 들어 영어 문장을 한국어로 번역할 때, 디코더가 '사과'라는 단어를 생성할 차례라면, 영어 원문에서 'apple'이라는 단어에 어텐션을 집중합니다.

### 2. 인코더의 자기-어텐션 (Self-Attention in Encoder)
- **무슨 역할을 하나요?**: 인코더에서는 문장의 각 단어가 서로의 관계를 확인하여, 어느 단어가 중요하고 관련이 있는지 학습합니다.
- **어떻게 동작하나요?**: 인코더 내의 모든 단어는 각기 다른 모든 단어와 상호작용하여 어텐션을 적용합니다. 즉, 한 단어가 문장의 다른 단어들을 모두 보고 어떤 관계가 있는지 살펴보는 과정입니다.
- **예시**: "나는 빨간 사과를 좋아해"라는 문장에서 '사과'라는 단어가 중요한데, 인코더는 '빨간'이라는 단어와의 관계도 이해해야 합니다. 이를 위해 '사과'는 '빨간'과 '좋아해'라는 단어들에 어텐션을 적용하여 전체 문맥을 파악합니다.

### 3. 디코더의 자기-어텐션 (Self-Attention in Decoder)
- **무슨 역할을 하나요?**: 디코더는 번역된 문장을 하나씩 생성할 때, 이전에 생성된 단어들만을 참고하여 다음 단어를 예측해야 합니다.
- **어떻게 동작하나요?**: 디코더는 현재까지 생성된 단어들에만 어텐션을 적용합니다. 다음 단어를 예측할 때, 이미 생성된 단어들 사이의 관계만을 고려하고, 앞으로 나올 단어들은 보지 못하게 막습니다.
- **오토레그레시브(Autoregressive)란?**: 디코더는 한 번에 하나씩 단어를 생성하는데, 이를 **오토레그레시브(Autoregressive)**라고 합니다. 이는 '미래의 단어는 모른다'는 방식입니다. 그래서 현재 단어까지의 정보만 참고하여 다음 단어를 생성합니다.
- **예시**: 디코더가 문장을 번역하면서 "나는"이라는 단어를 생성한 후 "빨간"을 생성할 차례라면, 이전에 생성된 '나는'을 참고하고, 아직 생성되지 않은 '사과'에 대해서는 정보를 알 수 없습니다.

### 요약하자면:
- **인코더-디코더 어텐션**: 디코더는 입력된 문장 전체를 보면서 중요한 단어에 어텐션을 적용하여 번역합니다.
- **인코더의 자기-어텐션**: 인코더는 문장 안의 각 단어가 서로 어떻게 연결되어 있는지 학습합니다.
- **디코더의 자기-어텐션**: 디코더는 번역된 문장을 한 단어씩 생성할 때, 이미 생성된 단어들만 참고합니다.

이 방식들이 합쳐져서 Transformer가 문장의 의미를 더 잘 이해하고 번역 등의 작업을 할 수 있게 됩니다.