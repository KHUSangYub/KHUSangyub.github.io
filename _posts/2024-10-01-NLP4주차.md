---
layout: post
title:  "But what is a GPT? Visual intro to transformers | Chapter 5, Deep Learning"
date:   2024-10-01 12:00:00 +0900
categories: [LLM]
tags: [DeepLearning , LLM, GPT,Transformer ]
math: true
---

# But what is a GPT? Visual intro to transformers | Chapter 5, Deep Learning

## Predict, sample , repeat

### GPT의 정의와 구조

- GPT: Generative Pre-trained Transformer의 약자
  - Generative: 새로운 텍스트를 생성하는 봇
  - Pre-trained: 대량의 데이터로 사전 학습됨
  - Transformer: 특정 유형의 신경망, 현재 AI 붐의 핵심 발명품

### Transformer의 다양한 응용

- 텍스트 생성
- 음성-텍스트 변환 및 그 반대
- 텍스트 기반 이미지 생성 (DALL-E, Midjourney 등)
- 언어 간 번역 (원래 Google이 2017년에 개발)

### GPT의 작동 원리

1. 텍스트 입력 받음 (이미지나 소리도 가능)
2. 다음에 올 텍스트에 대한 확률 분포 예측
3. 예측된 분포에서 무작위로 샘플링하여 새 텍스트 생성
4. 생성된 텍스트를 기존 텍스트에 추가
5. 전체 과정을 반복하여 긴 텍스트 생성

### GPT 모델의 발전

- GPT-2: 초기 모델, 간단한 텍스트 생성 가능
- GPT-3: 더 큰 규모, 더 정교한 텍스트 생성 가능

### Transformer의 내부 구조

1. 입력을 작은 조각(토큰)으로 분할
2. 각 토큰을 벡터로 변환 (임베딩)
3. 어텐션 블록을 통과: 벡터들이 서로 "대화"하며 정보 교환
4. 다층 퍼셉트론(MLP) 블록 통과: 각 벡터에 대한 개별 처리
5. 어텐션과 MLP 블록을 여러 번 반복
6. 최종 벡터로 다음 토큰 예측

### 주요 개념

- 컨텍스트 크기: 한 번에 처리할 수 있는 토큰의 수 (GPT-3는 2048)
- 소프트맥스 함수: 로짓을 확률 분포로 변환
- 온도(temperature): 텍스트 생성의 무작위성 조절

### 학습 방법

- 대규모 데이터로 사전 학습
- 특정 작업에 대해 추가 학습(파인튜닝) 가능


## Transformer의 내부 구조 상세 설명

### 1. 입력 처리
- **토큰화 (Tokenization)**
  - 입력 텍스트를 작은 단위(토큰)로 분할
  - 단어, 서브워드, 또는 문자 수준의 토큰화 가능
  - 예: "Hello, world!" → ["Hello", ",", "world", "!"]

- **임베딩 (Embedding)**
  - 각 토큰을 고정된 길이의 벡터로 변환
  - 일반적으로 256에서 1024 차원의 벡터 사용
  - 학습 가능한 파라미터로, 모델 훈련 중 최적화됨

### 2. Transformer 블록의 주요 구성 요소

#### a. 어텐션 블록 (Attention Block)
- **목적**: 각 토큰이 다른 모든 토큰과 어떻게 관련되는지 계산
- **작동 원리**:
  1. 각 토큰 벡터로부터 Query, Key, Value 벡터 생성
  2. Query와 Key의 내적을 통해 어텐션 점수 계산
  3. 점수를 정규화하고 Value에 가중치로 적용
- **멀티헤드 어텐션**:
  - 여러 개의 어텐션 메커니즘을 병렬로 실행
  - 다양한 관점에서 토큰 간 관계 포착 가능

#### b. 다층 퍼셉트론 (MLP) 블록
- **구조**: 일반적으로 2개의 선형 층과 활성화 함수로 구성
- **목적**: 
  - 비선형성 추가
  - 각 토큰의 표현을 개별적으로 변환
- **활성화 함수**: ReLU 또는 GELU 주로 사용

### 3. 처리 과정
1. 임베딩된 입력 벡터들이 어텐션 블록 통과
2. 어텐션 블록의 출력에 잔차 연결(residual connection) 적용
3. 레이어 정규화(layer normalization) 적용
4. 정규화된 출력이 MLP 블록 통과
5. MLP 블록의 출력에 다시 잔차 연결과 레이어 정규화 적용
6. 이 과정을 여러 번 반복 (보통 6~24개의 레이어)

### 4. 최종 출력 생성
- 마지막 레이어의 출력 벡터를 언임베딩(unembedding) 과정을 통해 변환
- 변환된 벡터를 어휘 크기의 로짓(logits)으로 해석
- 소프트맥스 함수를 적용하여 다음 토큰에 대한 확률 분포 생성

### 5. 주요 특징 및 개념
- **컨텍스트 크기 (Context Size)**
  - 한 번에 처리할 수 있는 최대 토큰 수
  - 모델의 메모리 요구사항과 직접적으로 연관
  - 예: GPT-3는 2048 토큰, 일부 최신 모델은 8192 토큰 이상 처리 가능

- **잔차 연결 (Residual Connections)**
  - 각 서브블록의 입력을 출력에 더함
  - 그래디언트 소실 문제 완화 및 학습 안정화

- **레이어 정규화 (Layer Normalization)**
  - 각 레이어의 출력을 정규화하여 학습 안정화
  - 배치 간 변동성 감소 및 수렴 속도 향상

- **위치 인코딩 (Positional Encoding)**
  - 토큰의 순서 정보를 모델에 제공
  - 사인/코사인 함수 또는 학습 가능한 임베딩 사용

### 6. 스케일링 및 성능
- 더 큰 모델은 더 많은 레이어, 더 큰 차원의 벡터, 더 많은 어텐션 헤드 사용
- 파라미터 수 증가에 따라 성능 향상, 그러나 계산 비용도 증가
- 모델 크기, 학습 데이터, 컴퓨팅 리소스 간의 균형 필요




## Chapter layout


### 1. 임베딩(Embeddings)
- **정의**: 단어나 토큰을 고정된 길이의 벡터로 변환하는 과정
- **목적**: 
  - 텍스트 데이터를 신경망이 처리할 수 있는 수치 형태로 변환
  - 단어 간의 의미론적 관계를 수학적으로 표현
- **주요 내용**:
  - 단어 임베딩의 기본 원리
  - 단어 벡터 간의 연산과 그 의미 (예: king - man + woman ≈ queen)
  - 서브워드 토크나이제이션 기법
  - 위치 인코딩(positional encoding)의 중요성과 구현 방법

### 2. 어텐션(Attention)
- **정의**: 입력 시퀀스의 각 요소가 다른 요소들과 어떻게 관련되는지 계산하는 메커니즘
- **목적**: 
  - 장거리 의존성 문제 해결
  - 입력 시퀀스의 중요한 부분에 집중할 수 있게 함
- **주요 내용**:
  - 셀프 어텐션(self-attention)의 작동 원리
  - 쿼리(Query), 키(Key), 밸류(Value) 개념 설명
  - 멀티헤드 어텐션(multi-head attention)의 구조와 이점
  - 어텐션 가중치 시각화 및 해석 방법

### 3. 다층 퍼셉트론(Multi-Layer Perceptron, MLP)
- **정의**: 여러 층의 뉴런으로 구성된 전방향(feedforward) 신경망
- **목적**: 
  - 비선형성 도입을 통한 모델의 표현력 증가
  - 복잡한 패턴 학습 능력 향상
- **주요 내용**:
  - MLP의 기본 구조와 각 층의 역할
  - 활성화 함수(ReLU, GELU 등)의 중요성과 선택 기준
  - 드롭아웃(dropout)과 같은 정규화 기법
  - Transformer에서 MLP 블록의 위치와 기능

### 4. 트레이닝(Training)
- **정의**: 모델의 파라미터를 최적화하여 원하는 작업을 수행할 수 있도록 하는 과정
- **목적**: 
  - 대규모 데이터셋을 사용하여 모델의 성능 향상
  - 과적합 방지와 일반화 능력 개선
- **주요 내용**:
  - 손실 함수 선택과 그 의미 (예: 교차 엔트로피)
  - 역전파 알고리즘의 상세한 작동 원리
  - 최적화 알고리즘 (Adam, AdamW 등)의 비교와 선택 기준
  - 학습률 스케줄링과 웜업(warmup) 전략
  - 배치 정규화(batch normalization)와 레이어 정규화(layer normalization)의 차이
  - 전이 학습(transfer learning)과 파인튜닝(fine-tuning) 기법

### 접근 방식 및 특징
- 각 챕터는 독립적으로 이해할 수 있지만, 순차적 학습을 통해 전체 아키텍처의 흐름 파악 가능
- 복잡한 수학적 개념을 직관적인 시각화와 비유를 통해 설명
- 실제 구현 코드보다는 핵심 아이디어와 원리에 초점
- 각 개념의 역사적 배경과 발전 과정도 함께 소개
- 실제 응용 사례와 최신 연구 동향을 포함하여 실용적 이해 도모

이 상세한 구조는 Transformer의 각 구성 요소를 깊이 있게 다루며, 학습자가 전체 시스템의 작동 원리를 체계적으로 이해할 수 있도록 돕습니다.


### 딥러닝의 전제 (The Premise of Deep Learning)

### 1. 딥러닝의 기본 개념

- **정의**: 여러 층의 인공 신경망을 사용하여 데이터로부터 패턴을 학습하는 기계학습의 한 분야
- **목적**: 복잡한 비선형 관계를 모델링하고 고차원 데이터에서 유용한 특징을 자동으로 추출

### 2. 신경망의 구조

- **뉴런 (Neuron)**:
  - 생물학적 뉴런에서 영감을 받은 기본 계산 단위
  - 입력값들의 가중합을 계산하고 활성화 함수를 통과시킴

- **층 (Layer)**:
  - 여러 뉴런의 집합
  - 입력층, 은닉층, 출력층으로 구성

- **가중치 (Weights)와 편향 (Biases)**:
  - 네트워크의 학습 가능한 파라미터
  - 입력 데이터의 중요도를 조절하는 역할

### 3. 딥러닝의 핵심 아이디어

- **표현 학습 (Representation Learning)**:
  - 원시 데이터로부터 유용한 특징을 자동으로 학습
  - 수동으로 특징을 설계할 필요 없이 데이터 자체에서 학습

- **계층적 특징 추출**:
  - 낮은 층에서는 간단한 특징을, 높은 층에서는 복잡한 특징을 학습
  - 예: 이미지 인식에서 에지 → 텍스처 → 물체 부분 → 전체 물체

- **end-to-end 학습**:
  - 입력부터 출력까지 모든 처리 단계를 통합적으로 최적화
  - 중간 단계의 수동 조정 없이 전체 파이프라인을 학습

### 4. 학습 과정

- **순전파 (Forward Propagation)**:
  - 입력 데이터를 네트워크의 각 층을 통과시켜 예측 생성

- **역전파 (Backpropagation)**:
  - 예측과 실제 값의 차이(오차)를 사용해 가중치 조정
  - 경사 하강법을 통해 손실 함수를 최소화

- **최적화 알고리즘**:
  - SGD(Stochastic Gradient Descent), Adam, RMSprop 등
  - 학습률, 모멘텀 등의 하이퍼파라미터 조정 필요

### 5. 딥러닝의 강점

- **비선형성 모델링**: 복잡한 패턴과 관계를 포착 가능
- **확장성**: 더 많은 데이터와 더 큰 모델로 성능 향상
- **전이 학습**: 사전 학습된 모델을 다른 작업에 적용 가능

### 6. 딥러닝의 도전 과제

- **대량의 데이터 필요**: 과적합 방지와 일반화를 위해 많은 데이터 요구
- **계산 비용**: 대규모 모델 학습에 많은 컴퓨팅 리소스 필요
- **해석 가능성**: 모델의 결정 과정을 이해하기 어려움 (블랙박스 문제)

### 7. Transformer와 딥러닝

- Transformer는 딥러닝의 원리를 자연어 처리에 적용한 혁신적 모델
- 어텐션 메커니즘을 통해 장거리 의존성 문제 해결
- 대규모 언어 모델(예: GPT, BERT)의 기반이 됨

### 8. 미래 전망

- 모델 효율성 향상: 더 적은 데이터와 계산으로 높은 성능 달성
- 다중 모달 학습: 텍스트, 이미지, 음성 등 다양한 데이터 유형 통합
- 윤리적 AI: 편향 감소, 공정성 향상, 해석 가능성 개선에 중점






## 단어 임베딩 (Word Embeddings)

### 1. 단어 임베딩의 정의
- 단어를 고정된 길이의 실수 벡터로 표현하는 기법
- 목적: 단어의 의미와 관계를 수치적으로 표현

### 2. 임베딩의 필요성
- 컴퓨터가 텍스트를 이해하고 처리할 수 있도록 변환
- 단어 간의 의미적, 문법적 관계를 수학적으로 포착

### 3. 임베딩의 특성
- **차원**: 일반적으로 50~300차원 사용, 모델 크기에 따라 더 큰 차원도 가능
- **밀집 표현**: 모든 차원에 의미 있는 값을 가짐 (원-핫 인코딩과 대조적)
- **연속성**: 유사한 단어는 벡터 공간에서 가까이 위치

### 4. 임베딩 생성 방법
- **Word2Vec**: 
  - CBOW(Continuous Bag of Words)와 Skip-gram 모델 사용
  - 주변 단어를 통해 중심 단어 예측, 또는 그 반대
- **GloVe (Global Vectors)**:
  - 전체 말뭉치의 동시 출현 통계를 활용
- **FastText**:
  - 서브워드 정보를 활용하여 미등록 단어 처리 가능
- **BERT, GPT 등 사전 학습 모델**:
  - 대규모 언어 모델의 중간 층 출력을 임베딩으로 사용

### 5. 임베딩의 특징과 장점
- **의미적 유사성 포착**: 
  - 유사한 의미의 단어들은 벡터 공간에서 가까이 위치
  - 예: "king"과 "queen"의 벡터 거리가 가까움
- **유추 관계 표현**:
  - 벡터 연산을 통해 단어 간 관계 표현 가능
  - 예: vec("king") - vec("man") + vec("woman") ≈ vec("queen")
- **다차원 의미 표현**:
  - 단어의 여러 의미나 특성을 다른 차원에 분산하여 표현

### 6. 임베딩 학습 과정
1. 대규모 텍스트 데이터 수집
2. 단어 또는 서브워드 토큰화
3. 학습 알고리즘 선택 (예: Skip-gram)
4. 목적 함수 최적화 (예: 네거티브 샘플링)
5. 반복적인 학습을 통해 임베딩 벡터 조정

### 7. 임베딩의 시각화
- t-SNE, PCA 등의 차원 축소 기법 사용
- 2D 또는 3D 공간에 단어들을 배치하여 관계 시각화

### 8. 임베딩의 평가
- **내재적 평가**: 
  - 단어 유사도 테스트, 유추 문제 해결 능력 측정
- **외재적 평가**: 
  - 실제 NLP 작업(분류, 번역 등)에서의 성능 평가

### 9. 컨텍스트 기반 임베딩
- BERT, ELMo 등의 모델에서 사용
- 단어의 문맥에 따라 동적으로 임베딩 생성
- 다의어 처리에 효과적

### 10. 임베딩의 한계와 주의점
- 편향(bias) 문제: 학습 데이터의 편향이 임베딩에 반영될 수 있음
- 문화적, 시간적 특성: 특정 시기나 문화권의 언어 사용을 반영
- 지속적인 업데이트 필요: 언어 사용의 변화를 반영하기 위해

### 11. Transformer에서의 임베딩
- 입력 토큰을 임베딩 벡터로 변환하는 첫 단계
- 위치 정보를 추가하기 위해 위치 인코딩과 결합

### 12. 미래 전망
- 다국어, 다중 모달 임베딩 발전
- 더 효율적이고 압축된 임베딩 기법 연구
- 편향 감소와 공정성 향상을 위한 연구

## Unembedding 

### 1. 언임베딩의 정의
- 임베딩 벡터를 다시 단어 또는 토큰 확률 분포로 변환하는 과정
- Transformer 모델의 출력 단계에서 중요한 역할 수행

### 2. 언임베딩의 목적
- 모델의 내부 표현(벡터)을 사람이 이해할 수 있는 형태(단어/토큰)로 변환
- 다음 토큰 예측을 위한 확률 분포 생성

### 3. 언임베딩 레이어의 구조
- 일반적으로 선형 변환(linear transformation) 사용
- 입력 차원: 모델의 은닉 상태 차원 (예: 768, 1024)
- 출력 차원: 어휘 크기 (예: 30,000, 50,000)

### 4. 언임베딩 과정
1. 모델의 최종 은닉 상태 벡터 입력 받음
2. 언임베딩 가중치 행렬과 행렬 곱 수행
3. 결과로 어휘 크기의 로짓(logits) 벡터 생성
4. (선택적) 소프트맥스 함수 적용하여 확률 분포로 변환

### 5. 수학적 표현
- 언임베딩: y = Wx + b
  - y: 출력 로짓 벡터 (어휘 크기)
  - W: 언임베딩 가중치 행렬
  - x: 입력 은닉 상태 벡터
  - b: 편향 (bias) 벡터

### 6. 가중치 공유 (Weight Tying)
- 임베딩 레이어와 언임베딩 레이어의 가중치를 공유하는 기법
- 장점:
  - 파라미터 수 감소 (메모리 효율성)
  - 일반화 성능 향상
  - 학습 안정성 개선
- 구현: 언임베딩 가중치 행렬을 임베딩 행렬의 전치(transpose)로 설정

### 7. 언임베딩의 중요성
- 모델의 예측을 해석 가능한 형태로 변환
- 다음 토큰 생성을 위한 핵심 단계
- 모델의 전체 성능에 직접적인 영향

### 8. 언임베딩과 온도 조절 (Temperature Scaling)
- 언임베딩 후 생성된 로짓에 온도 파라미터 적용
- 출력 분포의 엔트로피(다양성) 조절 가능
- 낮은 온도: 더 확실한(sharp) 분포, 높은 온도: 더 균일한 분포

### 9. 대규모 언어 모델에서의 언임베딩
- GPT, BERT 등의 모델에서 핵심 컴포넌트
- 어휘 크기가 매우 큰 경우 (50,000 이상) 계산 비용 증가
- 효율적인 구현 기법 필요 (예: 적응형 소프트맥스)

### 10. 언임베딩의 최적화 기법
- 희소 행렬 연산 활용
- GPU 가속을 위한 최적화된 CUDA 커널 사용
- 양자화(Quantization) 기법 적용하여 메모리 사용량 감소

### 11. 언임베딩의 평가
- 퍼플렉서티(Perplexity) 측정을 통한 모델 성능 평가
- 생성된 텍스트의 품질과 다양성 분석

### 12. 언임베딩의 도전 과제
- 대규모 어휘에 대한 효율적인 처리
- 희귀 단어나 미등록 단어(OOV) 처리
- 다국어 모델에서의 언어 간 균형 유지

### 13. 미래 연구 방향
- 더 효율적인 언임베딩 아키텍처 개발
- 문맥에 따른 동적 언임베딩 기법
- 멀티모달 출력을 위한 확장된 언임베딩 방법

이 정리는 언임베딩의 개념, 구조, 중요성, 구현 방법, 그리고 관련된 다양한 측면을 포괄적으로 다루고 있습니다. 학습 시 참고하기에 충분한 세부 정보를 포함하고 있으며, 언임베딩에 대한 깊이 있는 이해를 제공합니다.



## Softmax with Temperature (온도 조절 소프트맥스)

### 1. 기본 개념
- 소프트맥스 함수에 온도 파라미터를 추가한 변형
- 목적: 출력 확률 분포의 '날카로움(sharpness)' 또는 '부드러움(smoothness)' 조절

### 2. 수학적 정의
- 기본 소프트맥스: $\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$
- 온도 조절 소프트맥스: $\text{softmax}(x_i, T) = \frac{e^{x_i/T}}{\sum_j e^{x_j/T}}$
  - $T$: 온도 파라미터
  - $x_i$: i번째 로짓 값

### 3. 온도 파라미터의 영향
- $T < 1$: 분포가 더 날카로워짐 (높은 확률값은 더 높아지고, 낮은 확률값은 더 낮아짐)
- $T = 1$: 기본 소프트맥스와 동일
- $T > 1$: 분포가 더 부드러워짐 (확률값들이 서로 비슷해짐)
- $T \to 0$: 극단적으로 날카로워져 argmax 함수와 유사해짐
- $T \to \infty$: 균일 분포에 가까워짐

### 4. 사용 목적
- **텍스트 생성의 다양성 조절**:
  - 낮은 온도: 더 예측 가능하고 일관된 출력
  - 높은 온도: 더 다양하고 창의적인 출력
- **모델 캘리브레이션**: 모델의 신뢰도 조정
- **지식 증류(Knowledge Distillation)**: 큰 모델의 '소프트' 출력을 작은 모델로 전달

### 5. 구현 방법
```python
def softmax_with_temperature(logits, temperature):
    logits = logits / temperature
    exp_logits = np.exp(logits)
    return exp_logits / np.sum(exp_logits)
```

### 6. 텍스트 생성에서의 응용
- **그리디 탐색(Greedy Search)**: T ≈ 0, 항상 가장 높은 확률의 토큰 선택
- **빔 서치(Beam Search)**: 낮은 T 값 사용, 상위 k개의 가능성 유지
- **랜덤 샘플링**: T > 0, 확률에 따라 다음 토큰 무작위 선택
- **Top-k 샘플링**: 상위 k개 토큰 중에서만 샘플링
- **Nucleus (Top-p) 샘플링**: 누적 확률이 p를 넘는 최소 집합에서 샘플링

### 7. 온도 선택의 중요성
- 태스크와 요구사항에 따라 적절한 온도 선택 필요
- 일반적으로 0.5 ~ 1.5 범위의 값 사용
- 실험을 통한 최적 온도 탐색 권장

### 8. 장단점
- **장점**:
  - 출력의 다양성과 품질 간 균형 조절 가능
  - 모델 동작의 유연성 증가
- **단점**:
  - 추가적인 하이퍼파라미터 조정 필요
  - 부적절한 온도 설정 시 성능 저하 가능

### 9. 고급 기법
- **동적 온도 조절**: 문맥에 따라 온도를 자동으로 조정
- **앙상블 방법**: 여러 온도 설정의 결과를 조합
- **적응형 온도 스케줄링**: 생성 과정에서 온도를 점진적으로 변경

### 10. 연구 동향
- 최적의 온도 자동 선택 알고리즘 개발
- 다중 온도 레이어: 모델의 각 층에 다른 온도 적용
- 태스크 특화 온도 조절 전략 연구

### 11. 실제 적용 사례
- OpenAI의 GPT 모델: 텍스트 생성의 창의성 조절
- 구글의 BERT: 파인튜닝 단계에서의 지식 전달
- 챗봇 시스템: 응답의 다양성과 적절성 균형 유지

### 12. 주의사항
- 과도하게 높은 온도: 무의미한 텍스트 생성 위험
- 과도하게 낮은 온도: 반복적이고 단조로운 출력 위험
- 모델 평가 시 온도 설정 명시 필요

