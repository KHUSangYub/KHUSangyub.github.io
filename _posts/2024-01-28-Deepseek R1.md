---
layout: post
title:  "Building makemore Part2: MLP"
date:   2025-01-28 12:00:00 +0900
categories: [LLM]
tags: [DeepLearning , MLP]
math: true
---

# DeepSeek-R1: Incentivizing Reasoning Capability inLLMs via Reinforcement Learning

## INTRO

MOE 아키텍처 기반의 모델인 Deepseek-R1 모델은 Supervised-Fine-Tuning을 진행하지 않고 Reinforcement-Learning만을 통해 OpenAI-o1-1217의 Benchmark Performance와 유사하거나 그 이상의 성능에 도달했습니다.

![alt text](../assets/png/deepseek_benchmark.PNG)

**각 Benchmark 설명**  
- **AIME 2024 (Pass@1)**:  
  AIME(American Invitational Mathematics Examination)는 고급 수학 문제를 평가하는 시험입니다. 모델이 수학적 문제를 정확히 해결하는 능력을 평가하는 데 사용됩니다. Pass@1은 모델이 첫 시도에서 정답을 맞히는 비율을 나타냅니다.

- **Codeforces (Percentile)**:  
  Codeforces는 알고리즘 문제 해결 및 프로그래밍 대회를 위한 플랫폼입니다. Percentile은 모델의 성능이 참가자들 사이에서 몇 퍼센타 수준에 해당하는지를 나타냅니다.

- **GPQA Diamond (Pass@1)**:  
  GPQA(General Purpose Question Answering) Diamond는 고급 질의응답 태스크를 테스트하는 벤치마크입니다. Diamond는 난이도가 높은 질문들을 포함하며, Pass@1은 첫 번째 시도로 정확히 답을 맞힌 비율입니다.

- **MATH-500 (Pass@1)**:  
  MATH-500은 수학 문제 해결 능력을 테스트하는 데이터셋으로, 특히 고난도의 500개 문제를 포함합니다. Pass@1은 모델이 한 번에 정확히 답을 내는 성능을 측정합니다.

- **MMLU (Pass@1)**:  
  MMLU(Massive Multitask Language Understanding)는 여러 주제에 걸쳐 모델의 언어 이해 능력을 평가하는 벤치마크입니다. Pass@1은 첫 시도에서 정확히 답을 맞힌 비율입니다.

- **SWE-bench Verified (Resolved)**:  
  SWE-bench는 소프트웨어 엔지니어링 문제를 포함하는 벤치마크입니다. Resolved는 모델이 문제를 해결한 비율을 나타냅니다.


